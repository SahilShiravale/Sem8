%%writefile Application.cpp
#include <iostream>
#include <vector>
#include <omp.h>
using namespace std;

// Simple linear regression (y = mx + c)
void linearRegression(const vector<double>& X, const vector<double>& Y, double& m, double& c) {
    int n = X.size();
    double sumX = 0, sumY = 0, sumXY = 0, sumXX = 0;

    #pragma omp parallel for reduction(+:sumX, sumY, sumXY, sumXX)
    for (int i = 0; i < n; i++) {
        sumX += X[i];
        sumY += Y[i];
        sumXY += X[i] * Y[i];
        sumXX += X[i] * X[i];
    }

    // Calculate coefficients m and c using the normal equation
    m = (n * sumXY - sumX * sumY) / (n * sumXX - sumX * sumX);
    c = (sumY - m * sumX) / n;
}

int main() {
    vector<double> X = {1, 2, 3, 4, 5};  // Input features
    vector<double> Y = {2, 4, 5, 4, 5};  // Target labels
    double m, c;

    linearRegression(X, Y, m, c);

    cout << "Linear Regression Model: y = " << m << "x + " << c << endl;
    return 0;
}

//!g++ -fopenmp Application.cpp -o run
//!./run

The provided code implements Parallel Linear Regression using OpenMP to compute the slope (`m`) and intercept (`c`) of a linear regression model (`y = mx + c`) for given input features (`X`) and target labels (`Y`). It uses the Jupyter Notebook magic command `%%writefile Application.cpp` to save the code, followed by commands to compile (`g++ -fopenmp Application.cpp -o run`) and run (`./run`). Below, I’ll explain the code step by step, describe how parallelism is achieved, highlight benefits over sequential execution, confirm the use of existing algorithms, and address performance measurement.



 Step-by-Step Explanation of the Code

1. File Creation with `%%writefile`:
   - `%%writefile Application.cpp` saves the code to `Application.cpp` in the Jupyter Notebook’s working directory.
   - This enables compilation and execution later.

2. Header Files and Namespace:
   - Includes: `<iostream>`, `<vector>`, `<omp.h>` for I/O, dynamic arrays, and OpenMP parallelism.
   - `using namespace std;`: Avoids prefixing standard library components (e.g., `std::cout`).

3. Linear Regression Function (`linearRegression`):
   - Parameters:
     - `const vector<double>& X`: Input features (independent variable).
     - `const vector<double>& Y`: Target labels (dependent variable).
     - `double& m`: Slope of the regression line (output).
     - `double& c`: Intercept of the regression line (output).
   - Logic:
     - Initialization:
       - `n = X.size()`: Number of data points.
       - `sumX`, `sumY`, `sumXY`, `sumXX`: Variables to store sums needed for regression (initialized to 0).
     - Parallel Reduction:
       - Directive: `#pragma omp parallel for reduction(+:sumX, sumY, sumXY, sumXX)`.
       - Loop: Iterates over indices `i` from 0 to `n-1`.
       - Computations:
         - `sumX += X[i]`: Sum of X values.
         - `sumY += Y[i]`: Sum of Y values.
         - `sumXY += X[i] * Y[i]`: Sum of products of X and Y.
         - `sumXX += X[i] * X[i]`: Sum of squares of X.
       - The `reduction(+)` clause ensures thread-safe accumulation of sums.
     - Coefficient Calculation:
       - Uses the normal equation for linear regression:
         - Slope: `m = (n * sumXY - sumX * sumY) / (n * sumXX - sumX * sumX)`.
         - Intercept: `c = (sumY - m * sumX) / n`.
       - These formulas minimize the sum of squared errors for `y = mx + c`.

4. Main Function:
   - Setup:
     - Defines `X = {1, 2, 3, 4, 5}` (input features).
     - Defines `Y = {2, 4, 5, 4, 5}` (target labels).
     - Declares `m` and `c` to store regression coefficients.
   - Execution:
     - Calls `linearRegression(X, Y, m, c)` to compute `m` and `c`.
     - Prints the model: `Linear Regression Model: y = <m>x + <c>`.
   - Example Output (approximate, based on data):
     ```
     Linear Regression Model: y = 0.6x + 2
     ```
     (Computed from normal equations for the given `X` and `Y`.)

5. Compilation and Execution:
   - `!g++ -fopenmp Application.cpp -o run`: Compiles the code with OpenMP support, creating an executable `run`.
   - `!./run`: Runs the executable, printing the regression model.
   - Note: No performance measurement (e.g., `omp_get_wtime()`) is included.



 How Parallelism is Achieved
- Mechanism:
  - Directive: `#pragma omp parallel for reduction(+:sumX, sumY, sumXY, sumXX)`.
  - Parallel For: Splits the loop (`i = 0` to `n-1`) across threads.
    - Example: For `n = 5` with 4 threads, each thread processes a subset of indices (e.g., thread 0 handles `i = 0, 1`, thread 1 handles `i = 2`, etc.).
  - Reduction:
    - Each thread computes local sums for `sumX`, `sumY`, `sumXY`, and `sumXX`.
    - OpenMP combines local sums using addition (`+`) to produce global sums.
  - Thread Safety:
    - The `reduction(+)` clause ensures thread-safe accumulation without race conditions.
    - Each thread has a private copy of the sum variables, merged at the end.
- Process:
  - Threads process array elements concurrently, computing partial sums.
  - OpenMP merges results to compute final sums, used for `m` and `c`.



 Benefits of Parallelism vs. Sequential Execution

# General Benefits
- Speedup:
  - Parallel execution leverages multiple CPU cores, reducing computation time for large datasets.
  - Sequential execution uses one core, processing all elements linearly.
- Scalability:
  - Parallel version scales with dataset size and core count, ideal for HPC.
  - Sequential version slows linearly with input size (`n`).

# Specific Benefits
- Parallel Reduction:
  - Benefit: Divides the computation of `sumX`, `sumY`, `sumXY`, `sumXX` across threads.
    - Example: For `n = 5` with 4 threads, each thread processes ~1-2 elements, potentially reducing time by up to 4x (ideal case).
  - Sequential Reduction: Processes all `n` elements one by one, taking longer for large `n`.
- Efficiency:
  - Reduction clauses optimize combining results, minimizing synchronization overhead.
  - For small datasets (like `n = 5` in the code), speedup is minimal due to thread creation overhead.
- Large Datasets:
  - For large `n` (e.g., 10000 points), parallelism significantly reduces time, as threads process more elements concurrently.
  - Example: Sequential might take 0.001s, parallel 0.0003s with 4 cores (hypothetical).

# Limitations
- Small Dataset (`n = 5`):
  - Parallel overhead (thread creation, reduction combining) may make sequential faster.
  - The code’s small size limits observable speedup.
- Comparison to BFS/DFS:
  - Linear regression reduction is highly parallelizable (like Min/Max/Sum), offering better speedup than DFS (sequential nature) or BFS (synchronization overhead).



 Use of Existing Algorithms
- Algorithm: Linear regression using the normal equation method.
  - Computes `m` and `c` by minimizing squared errors.
  - Requires sums: `sumX`, `sumY`, `sumXY`, `sumXX`.
- Sequential Equivalent: Standard iterative summation (loop over data points).
- Parallel: OpenMP reduction for sums, a standard HPC technique.
- Relevance: Linear regression is a fundamental machine learning algorithm, and parallel reduction is common in HPC for data aggregation.



 Performance Measurement
- Limitation: The code does not measure performance (no `omp_get_wtime()` for timing).
- Implication: Cannot quantify parallel speedup without timing sequential and parallel executions.
- Expected Behavior:
  - For `n = 5`, sequential may be faster due to overhead.
  - For large `n`, parallel would show speedup (e.g., 2-4x faster with 4 cores).
- Improvement Needed:
  - Add sequential implementation and `omp_get_wtime()` for timing.
  - Use larger datasets to demonstrate parallelism benefits.



 Compilation and Execution
- In Jupyter Notebook:
  - Run the cell with `%%writefile Application.cpp` to save the code.
  - Compile and run in separate cells:
    ```jupyter
    !g++ -fopenmp Application.cpp -o run
    !./run
    ```
  - Output (approximate):
    ```
    Linear Regression Model: y = 0.6x + 2
    ```

- In Terminal:
  - Save the code to `Application.cpp` (omit `%%writefile`).
  - Compile and run:
    ```bash
    g++ -fopenmp Application.cpp -o run
    ./run
    ```



 Limitations and Improvements
- Limitations:
  - No performance measurement (no `omp_get_wtime()`).
  - No sequential implementation for comparison.
  - Fixed small dataset (`n = 5`) limits parallelism benefits.
  - No input validation or user input for `X` and `Y`.
- Improvements:
  - Add sequential regression function and `omp_get_wtime()` for timing.
  - Allow user input for dataset size and values.
  - Test with larger datasets to show speedup.
  - Add error handling (e.g., check if `X` and `Y` have same size).



 Relevance to HPC and Deep Learning
- High Performance Computing:
  - OpenMP parallel reduction optimizes summation for multi-core CPUs, a core HPC technique.
  - Scales well for large datasets, common in scientific computing.
- Deep Learning:
  - Linear regression is a foundational machine learning algorithm.
  - Parallel reduction is relevant for computing gradients or statistics in large-scale ML models.
- Performance Potential: With timing and larger datasets, the code could demonstrate significant parallel speedup.



 Summary
- Code: Computes linear regression coefficients (`m`, `c`) using OpenMP parallel reduction for sums.
- Parallelism:
  - `#pragma omp parallel for reduction(+)` splits summation across threads.
  - Thread-safe aggregation via reduction clauses.
- Benefits:
  - Faster for large datasets by distributing work across cores.
  - Limited benefit for small datasets (`n = 5`) due to overhead.
- Execution: Save with `%%writefile`, compile (`g++ -fopenmp`), run (`./run`).

If you want a version with sequential implementation, performance timing, or user input, let me know!