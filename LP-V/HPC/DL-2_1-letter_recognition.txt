import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# 1) Load & preprocess data
df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data",
                 header=None)
df.columns = ['letter'] + [f'feat{i}' for i in range(1,17)]
le = LabelEncoder()
y_int = le.fit_transform(df['letter'])
y = to_categorical(y_int)
X = StandardScaler().fit_transform(df.drop('letter', axis=1))
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 2) Build & train model
model = Sequential([
    Input(shape=(16,)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(26, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)

# 3) Predict & evaluate
y_pred = np.argmax(model.predict(X_test), axis=1)
y_true = np.argmax(y_test, axis=1)
print("\nClassification Report:\n")
print(classification_report(y_true, y_pred, target_names=le.inverse_transform(range(26))))

# 4) Plot accuracy & loss
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss')
plt.legend()
plt.tight_layout()
plt.show()


This Python code implements a Deep Neural Network (DNN) for multi-class classification on the Letter Recognition dataset, predicting one of 26 letters (A-Z) from 16 numerical features. It uses libraries like Pandas, NumPy, Scikit-learn, TensorFlow/Keras, and Matplotlib. The code is divided into 4 blocks, each explained below in simple language, covering what is used, its purpose, and key details, with a focus on parallelism and benefits over sequential execution.



 Block-by-Block Explanation

# 1. Load & Preprocess Data
```python
df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data", header=None)
df.columns = ['letter'] + [f'feat{i}' for i in range(1,17)]
le = LabelEncoder()
y_int = le.fit_transform(df['letter'])
y = to_categorical(y_int)
X = StandardScaler().fit_transform(df.drop('letter', axis=1))
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```
- What: Loads the Letter Recognition dataset, preprocesses it, and splits it.
- Used:
  - `pd.read_csv`: Downloads and reads CSV from a URL into a DataFrame.
  - `df.columns`: Names columns (`letter` for target, `feat1` to `feat16` for features).
  - `LabelEncoder` (Scikit-learn): Converts letters (A-Z) to integers (0-25).
  - `to_categorical` (Keras): Converts integers to one-hot encoded vectors (e.g., 0 → [1,0,...,0]).
  - `StandardScaler`: Standardizes features to mean 0, variance 1.
  - `train_test_split`: Splits data into 80% training, 20% testing.
- Purpose: Prepares features (`X`) and one-hot encoded labels (`y`) for training.
- Why: Clean, normalized data and proper label encoding are essential for DNN training.

# 2. Build & Train Model
```python
model = Sequential([
    Input(shape=(16,)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(26, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)
```
- What: Creates and trains a DNN for classification.
- Used:
  - `Sequential` (Keras): Defines a sequential model.
  - `Input`: Sets input size (16 features).
  - `Dense`: Adds layers:
    - 64 neurons (ReLU activation) for feature learning.
    - 32 neurons (ReLU) for deeper patterns.
    - 26 neurons (softmax) for 26-class probabilities.
  - `compile`: Uses Adam optimizer, categorical crossentropy loss, and accuracy metric.
  - `fit`: Trains model on `X_train`, `y_train` for 10 epochs, with 20% validation split.
  - `history`: Stores training/validation accuracy and loss.
- Purpose: Learns to predict letters from features.
- Why: DNN with multiple layers captures complex patterns; softmax outputs class probabilities.

# 3. Predict & Evaluate
```python
y_pred = np.argmax(model.predict(X_test), axis=1)
y_true = np.argmax(y_test, axis=1)
print("\nClassification Report:\n")
print(classification_report(y_true, y_pred, target_names=le.inverse_transform(range(26))))
```
- What: Makes predictions and evaluates model performance.
- Used:
  - `model.predict`: Generates probabilities for test set.
  - `np.argmax`: Converts probabilities to class indices (0-25).
  - `classification_report` (Scikit-learn): Computes precision, recall, F1-score per class.
  - `le.inverse_transform`: Converts indices back to letters (A-Z).
- Purpose: Assesses how well the model classifies letters.
- Why: Metrics show accuracy and per-class performance (e.g., precision for letter A).

# 4. Plot Accuracy & Loss
```python
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy')
plt.legend()
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss')
plt.legend()
plt.tight_layout()
plt.show()
```
- What: Plots training and validation accuracy/loss over epochs.
- Used:
  - `matplotlib.pyplot`: Creates two subplots.
  - `history.history`: Accesses accuracy/loss data from training.
  - `plot`, `subplot`, `title`, `legend`, `tight_layout`, `show`: Configures plots.
- Purpose: Visualizes model performance over time.
- Why: Helps identify overfitting (e.g., high train accuracy, low validation accuracy).



 How Parallelism is Achieved
- No Explicit Parallelism:
  - The code uses TensorFlow/Keras, which employs implicit parallelism via:
    - Vectorization: Matrix operations (e.g., Dense layer computations) are optimized in C++.
    - Backend: TensorFlow may use multi-core CPU or GPU for matrix multiplications and gradient computations.
  - Scikit-learn’s `StandardScaler` and `classification_report` use optimized C-based computations.
  - No OpenMP or explicit multi-threading (unlike previous codes with `#pragma omp`).
- Mechanism:
  - Training (`model.fit`): Batch processing and gradient updates are parallelized by TensorFlow (e.g., BLAS for matrix ops).
  - Prediction (`model.predict`): Forward passes are vectorized, potentially multi-core.
  - Preprocessing: Scikit-learn’s scaling may use optimized loops.
- Dataset Size: 20,000 samples with 16 features; parallelism benefits are moderate due to modest size.



 Benefits of Parallelism vs. Sequential Execution
- Implicit Parallelism Benefits:
  - Speedup: TensorFlow’s vectorized operations are faster than sequential Python loops (e.g., for gradient descent or predictions).
  - Scalability: Scales to larger datasets or models, leveraging CPU/GPU cores.
  - Example: Training 10 epochs on 16,000 training samples (80% of 20,000) is faster with TensorFlow than a Python loop implementing backpropagation.
- Specific to Code:
  - Moderate Dataset (20,000 samples): Parallelism speeds up training and prediction, but benefits are limited by small feature count (16) and model size (64+32+26 neurons).
  - Sequential Equivalent: A Python loop for forward/backward passes would be significantly slower, especially for large datasets.
  - HPC Context: TensorFlow’s parallelism is HPC-relevant for large-scale ML (e.g., training deep networks on big data).
- Quantitative:
  - Training might take ~5-10s on CPU; sequential implementation could take minutes for similar tasks.



 Use of Existing Algorithms
- Algorithm: Multi-class classification via DNN.
  - Model: 3-layer DNN (64 ReLU, 32 ReLU, 26 softmax) for 26-class prediction.
  - Training: Gradient descent with Adam optimizer, categorical crossentropy loss.
- Sequential Equivalent: Manual gradient descent or simpler classifiers (e.g., logistic regression).
- Parallel: TensorFlow’s vectorized operations and potential multi-core/GPU parallelism.
- Relevance: DNNs are standard in ML for complex tasks; parallelism is HPC-relevant.



 Performance Measurement
- Metrics:
  - Accuracy: Reported during training (via `metrics=['accuracy']`).
  - Classification Report: Precision, recall, F1-score per class (e.g., `precision: 0.95` for letter A).
  - Plots: Visualize train/validation accuracy and loss to assess convergence/overfitting.
- No Timing Measurement:
  - Lacks `time.time()` to measure training/prediction time.
  - Implicit parallelism isn’t quantified.
- Improvement: Add timing to compare with sequential implementation.



 Execution
- In Jupyter Notebook:
  - Run the code directly after installing dependencies:
    ```bash
    pip install pandas numpy scikit-learn tensorflow matplotlib
    ```
  - Output: Classification report and two plots (accuracy/loss).
- In Terminal:
  - Save as `letter_recognition.py` (without `%%writefile`).
  - Run:
    ```bash
    python letter_recognition.py
    ```



 Limitations and Improvements
- Limitations:
  - Moderate dataset (20,000 samples) limits parallelism benefits.
  - No timing to quantify performance.
  - Fixed hyperparameters (e.g., 10 epochs, 64/32 neurons).
  - No data augmentation or cross-validation.
- Improvements:
  - Add `time.time()` for training/prediction timing.
  - Tune hyperparameters (e.g., layers, epochs).
  - Add cross-validation for robust evaluation.
  - Compare with simpler models (e.g., SVM).



 Relevance to HPC and Deep Learning
- High Performance Computing:
  - TensorFlow’s implicit parallelism (vectorization, CPU/GPU) aligns with HPC for large-scale ML.
  - Moderate dataset limits benefits here.
- Deep Learning:
  - DNN for letter recognition is a classic ML task.
  - Framework scales to complex tasks (e.g., image/text classification).



 Summary
- Code: Classifies letters (A-Z) using a DNN on the Letter Recognition dataset.
- Parallelism: Implicit via TensorFlow’s vectorized operations, potentially multi-core/GPU.
- Benefits: Faster than sequential loops for large data; moderate benefit for 20,000 samples.
- Execution: Run in Jupyter or as Python script; no compilation needed.
- Use: Demonstrates ML workflow (preprocessing, DNN training, evaluation, visualization).

If you want timing, hyperparameter tuning, or comparison with sequential methods, let me know!